{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide terms by the starting character\n",
    "df_term_a = df_insurance_terms_2[df_insurance_terms_2['term'].str.startswith('a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def normalize_string(string):\n",
    "    string = string.lower()\n",
    "    tokenized_words = word_tokenize(string)\n",
    "    stopwords_removed = [word for word in tokenized_words if word not in stop_words]\n",
    "    alphabet_string = [word for word in stopwords_removed if word.isalpha()]\n",
    "    return alphabet_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_a.loc[:, 'term'] = df_term_a.apply(lambda row: normalize_string(row['term']), axis=1)\n",
    "df_term_a.loc[:, 'text'] = df_term_a.apply(lambda row: normalize_string(row['text']), axis=1)\n",
    "\n",
    "df_term_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case\n",
    "# df_term_a['term'] = df_term_a['term'].str.lower()\n",
    "# df_term_a['text'] = df_term_a['text'].str.lower()\n",
    "df_term_a.loc[:,'term'] = df_term_a['term'].str.lower()\n",
    "df_term_a.loc[:,'text'] = df_term_a['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from nltk import word_tokenize\n",
    "# df_term_a['term_tokenized'] = df_term_a.apply(lambda row: word_tokenize(row['term']), axis=1)\n",
    "# df_term_a['text_tokenized'] = df_term_a.apply(lambda row: word_tokenize(row['text']), axis=1)\n",
    "df_term_a.loc[:, 'term_tokenized'] = df_term_a['term'].apply(word_tokenize)\n",
    "df_term_a.loc[:, 'text_tokenized'] = df_term_a['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_remove_stopwords = df_term_a.loc[0, 'text_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# df_term_a['term_tokenized'] = df_term_a['term_tokenized'].apply(lambda x: [item for item in x if item not in stop])\n",
    "# df_term_a['text_tokenized'] = df_term_a['text_tokenized'].apply(lambda x: [item for item in x if item not in stop])\n",
    "df_term_a.loc[:,'term_tokenized'] = df_term_a['term_tokenized'].apply(lambda x: [item for item in x if item not in stop])\n",
    "df_term_a.loc[:,'text_tokenized'] = df_term_a['text_tokenized'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_remove_stopwords = df_term_a.loc[0, 'text_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in before_remove_stopwords:\n",
    "    if item not in after_remove_stopwords:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only alphabet\n",
    "# df_term_a['term_tokenized'] = df_term_a['term_tokenized'].apply(lambda x: [item for item in x if item.isalpha()])\n",
    "# df_term_a['text_tokenized'] = df_term_a['text_tokenized'].apply(lambda x: [item for item in x if item.isalpha()])\n",
    "df_term_a.loc[:,'term_tokenized'] = df_term_a['term_tokenized'].apply(lambda x: [item for item in x if item.isalpha()])\n",
    "df_term_a.loc[:,'text_tokenized'] = df_term_a['text_tokenized'].apply(lambda x: [item for item in x if item.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = documents.values\n",
    "y = labels.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "vec_trained = vectorizer.fit_transform(X_train)\n",
    "\n",
    "vec_trained = vec_trained.toarray()\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "vec_test = vectorizer.transform(X_test)\n",
    "\n",
    "vec_test = vec_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_trained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there 3261 terms corresponding to 3261 labels. First I categorize the label into different clusters. And I want to know how many unique words in documents and labels? So I can decide the size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for index, text in documents.iteritems():\n",
    "#     print(text)\n",
    "    docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized = [word_tokenize(text.lower()) for text in docs]\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "docs = [[word for word in words if  word not in stop] for words in tokenized] # words 是一個列 word 是該列的一個單字\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[0]\n",
    "\n",
    "vocab = [word for words in docs for word in words]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "vocab = [wordnet.lemmatize(word) for word in vocab]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))\n",
    "\n",
    "vocab = set(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8067 unique words in document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap above into function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def get_vocabulary(docs):\n",
    "    tokenized = [word_tokenize(text.lower()) for text in docs]\n",
    "    stop = set(stopwords.words('english'))\n",
    "    docs = [[word for word in words if word not in stop] for words in tokenized]\n",
    "    vocab = [word for words in docs for word in words]\n",
    "\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    vocab = [wordnet.lemmatize(word) for word in vocab]\n",
    "    vocab = set(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "texts = []\n",
    "\n",
    "for index, row in df_insurance_terms_2.iterrows():\n",
    "#     print(index, row[0], row[1])\n",
    "    terms.append(row[0])\n",
    "    texts.append(row[1])\n",
    "    \n",
    "vocab_terms = get_vocabulary(terms)\n",
    "vocab_texts = get_vocabulary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab_terms), len(vocab_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 2873 unique words in terms and 8067 unique words in texts.\n",
    "Let me set vocabulary for terms and texts to be (1/3)*size in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_terms = 2873//3\n",
    "max_size_texts = 8067//3\n",
    "print(max_size_terms, max_size_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = documents.values\n",
    "y = labels.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_X = TfidfVectorizer(stop_words='english', max_features=max_size_texts)\n",
    "vec_X_trained = vectorizer_X.fit_transform(X_train)\n",
    "\n",
    "vectorizer_y = TfidfVectorizer(stop_words='english', max_features=max_size_terms)\n",
    "vec_y_trained = vectorizer_y.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_X = vectorizer_X.get_feature_names()\n",
    "vocab_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_y = vectorizer_y.get_feature_names()\n",
    "vocab_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use grid search to find the optimized k for KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_labels = vectorizer_y.transform(labels)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "n=8\n",
    "kmeans = KMeans(n_clusters=n).fit(vec_trained)\n",
    "print(kmeans.labels_.size)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "_count_vectorizer_labels = CountVectorizer()\n",
    "_count_vectorizer_defs = CountVectorizer()\n",
    "\n",
    "labels=['battery',\n",
    "        'car, van, suv',\n",
    "        'imaginary thing']\n",
    "defs=['device that uses chemicals to store an electric charge',\n",
    "      'passenger vehicle with 4 wheels and no truck bed',\n",
    "      'device that stores a vehicle or chemicals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectors_labels = _count_vectorizer_labels.fit_transform(labels).todense()\n",
    "count_vectors_defs = _count_vectorizer_defs.fit_transform(defs).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 1, 0, 1],\n",
       "        [0, 0, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectors_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
       "        [0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]],\n",
       "       dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectors_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectors_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 19)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectors_defs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = 2\n",
    "labels_ct = ([count_vectors_labels[row,n] for n in range(count_vectors_labels.shape[1])])\n",
    "\n",
    "labels_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['battery', 'car', 'imaginary', 'suv', 'thing', 'van']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_feature = (_count_vectorizer_labels.get_feature_names())\n",
    "\n",
    "labels_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tbattery\n",
      "0\tcar\n",
      "1\timaginary\n",
      "0\tsuv\n",
      "1\tthing\n",
      "0\tvan\n"
     ]
    }
   ],
   "source": [
    "z = zip(labels_ct, labels_feature)\n",
    "for ct,feat in z:\n",
    "   print(f'{ct}\\t{feat}')\n",
    "\n",
    "# 0 battery\n",
    "# 0 car\n",
    "# 1 imaginary\n",
    "# 0 suv\n",
    "# 1 thing\n",
    "# 0 van\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = 2\n",
    "defs_ct = ([count_vectors_defs[row,n] for n in range(count_vectors_defs.shape[1])])\n",
    "\n",
    "defs_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'and',\n",
       " 'bed',\n",
       " 'charge',\n",
       " 'chemicals',\n",
       " 'device',\n",
       " 'electric',\n",
       " 'no',\n",
       " 'or',\n",
       " 'passenger',\n",
       " 'store',\n",
       " 'stores',\n",
       " 'that',\n",
       " 'to',\n",
       " 'truck',\n",
       " 'uses',\n",
       " 'vehicle',\n",
       " 'wheels',\n",
       " 'with']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defs_feature = (_count_vectorizer_defs.get_feature_names())\n",
    "\n",
    "defs_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tan\n",
      "0\tand\n",
      "0\tbed\n",
      "0\tcharge\n",
      "1\tchemicals\n",
      "1\tdevice\n",
      "0\telectric\n",
      "0\tno\n",
      "1\tor\n",
      "0\tpassenger\n",
      "0\tstore\n",
      "1\tstores\n",
      "1\tthat\n",
      "0\tto\n",
      "0\ttruck\n",
      "0\tuses\n",
      "1\tvehicle\n",
      "0\twheels\n",
      "0\twith\n"
     ]
    }
   ],
   "source": [
    "z = zip(defs_ct, defs_feature)\n",
    "for ct,feat in z:\n",
    "   print(f'{ct}\\t{feat}')\n",
    "\n",
    "# 0 an\n",
    "# 0 and\n",
    "# 0 bed\n",
    "# 0 charge\n",
    "# 1 chemicals\n",
    "# 1 device\n",
    "# 0 electric\n",
    "# 0 no\n",
    "# 1 or\n",
    "# 0 passenger\n",
    "# 0 store\n",
    "# 1 stores\n",
    "# 1 that\n",
    "# 0 to\n",
    "# 0 truck\n",
    "# 0 uses\n",
    "# 1 vehicle\n",
    "# 0 wheels\n",
    "# 0 with\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "defs_labels_regressor = MLPRegressor()\n",
    "defs_labels_regressor.fit(count_vectors_defs, count_vectors_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_in_label(inpt):\n",
    "   inpt_len = len(inpt.split(' '))\n",
    "   pred = defs_labels_regressor.predict(_count_vectorizer_defs.transform([inpt]))\n",
    "   pred = [pred[0,n] for n in range(pred.shape[1])]\n",
    "   z = zip(labels_feature, pred)\n",
    "   for feat,prob in z:\n",
    "       print(f'{feat:20}{prob:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "battery             0.7931\n",
      "car                 0.0081\n",
      "imaginary           -0.0939\n",
      "suv                 -0.0501\n",
      "thing               0.1312\n",
      "van                 0.0078\n"
     ]
    }
   ],
   "source": [
    "inpt = 'device that uses chemicals to store an exciting charge'\n",
    "words_in_label(inpt)\n",
    "\n",
    "# battery             0.6707\n",
    "# car                 0.0414\n",
    "# imaginary           0.0636\n",
    "# suv                 0.0169\n",
    "# thing               0.0724\n",
    "# van                 -0.0054\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "battery             -0.0194\n",
      "car                 0.8085\n",
      "imaginary           0.0579\n",
      "suv                 0.8058\n",
      "thing               0.0993\n",
      "van                 0.6642\n"
     ]
    }
   ],
   "source": [
    "inpt = 'passenger vehicle with 4 feet and no truck bed'\n",
    "words_in_label(inpt)\n",
    "\n",
    "# battery             -0.0071\n",
    "# car                 1.0766\n",
    "# imaginary           0.0989\n",
    "# suv                 0.8496\n",
    "# thing               0.1142\n",
    "# van                 1.0061\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "battery             0.4998\n",
      "car                 0.1810\n",
      "imaginary           -0.1432\n",
      "suv                 0.4232\n",
      "thing               0.5352\n",
      "van                 0.6950\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inpt = 'device that uses chemicals and no bed'\n",
    "words_in_label(inpt)\n",
    "\n",
    "# battery             0.5551\n",
    "# car                 0.2812\n",
    "# imaginary           0.4854\n",
    "# suv                 0.0997\n",
    "# thing               0.7767\n",
    "# van                 0.8325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=1.0, penalty='l2')\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The dimension of X_train is (2282, 7400) and y_train is (2282, 2716)\n",
    "\n",
    "But in order to apply logistic regression, the y_train should be (2282,)\n",
    "\n",
    "I need to split the tokens in a row into different rows with same text.\n",
    "\n",
    "|norm term              | norm text                                           |\n",
    "|:---------------------:|:---------------------------------------------------:|\n",
    "|automatic premium loan\t| an optional provision in life insurance that a...   |\n",
    "\n",
    "should become \n",
    "\n",
    "\n",
    "|norm term              | norm text                                           |\n",
    "|:---------------------:|:---------------------------------------------------:|\n",
    "|automatic premium loan\t| an optional provision in life insurance that a...   |\n",
    "|premium                | an optional provision in life insurance that a...   |\n",
    "|loan                \t| an optional provision in life insurance that a...   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the normalized term and text to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt_norm = df_cnt[['norm_term', 'norm_text']]\n",
    "df_cnt_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnt_norm.to_csv('norm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
